# coding=utf-8
# pylint: disable=too-many-lines
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

import datetime
from typing import Any, List, Mapping, Optional, overload

from .. import _model_base
from .._model_base import rest_field


class BatchJob(_model_base.Model):  # pylint: disable=too-many-instance-attributes
    """An Azure Batch Job.

    Readonly variables are only populated by the server, and will be ignored when sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar id: The ID is case-preserving and case-insensitive (that is, you may not have two
     IDs within an Account that differ only by case). Required.
    :vartype id: str
    :ivar display_name: The display name for the Job.
    :vartype display_name: str
    :ivar uses_task_dependencies: Whether Tasks in the Job can define dependencies on each other.
     The default is
     false.
    :vartype uses_task_dependencies: bool
    :ivar url: The URL of the Job.
    :vartype url: str
    :ivar e_tag: This is an opaque string. You can use it to detect whether the Job has changed
     between requests. In particular, you can be pass the ETag when updating a Job
     to specify that your changes should take effect only if nobody else has
     modified the Job in the meantime.
    :vartype e_tag: str
    :ivar last_modified: This is the last time at which the Job level data, such as the Job state
     or
     priority, changed. It does not factor in task-level changes such as adding new
     Tasks or Tasks changing state.
    :vartype last_modified: ~datetime.datetime
    :ivar creation_time: The creation time of the Job.
    :vartype creation_time: ~datetime.datetime
    :ivar state: The state of the Job.
    :vartype state: str
    :ivar state_transition_time: The time at which the Job entered its current state.
    :vartype state_transition_time: ~datetime.datetime
    :ivar previous_state: This property is not set if the Job is in its initial Active state.
    :vartype previous_state: str
    :ivar previous_state_transition_time: This property is not set if the Job is in its initial
     Active state.
    :vartype previous_state_transition_time: ~datetime.datetime
    :ivar priority: Priority values can range from -1000 to 1000, with -1000 being the lowest
     priority and 1000 being the highest priority. The default value is 0.
    :vartype priority: int
    :ivar allow_task_preemption: If the value is set to True, other high priority jobs submitted to
     the system
     will take precedence and will be able requeue tasks from this job. You can
     update a job's allowTaskPreemption after it has been created using the update
     job API.
    :vartype allow_task_preemption: bool
    :ivar max_parallel_tasks: The value of maxParallelTasks must be -1 or greater than 0 if
     specified. If not
     specified, the default value is -1, which means there's no limit to the number
     of tasks that can be run at once. You can update a job's maxParallelTasks after
     it has been created using the update job API.
    :vartype max_parallel_tasks: int
    :ivar constraints: The execution constraints for a Job.
    :vartype constraints: str
    :ivar job_manager_task: The Job Manager Task is automatically started when the Job is created.
     The
     Batch service tries to schedule the Job Manager Task before any other Tasks in
     the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where
     Job Manager Tasks are running for as long as possible (that is, Compute Nodes
     running 'normal' Tasks are removed before Compute Nodes running Job Manager
     Tasks). When a Job Manager Task fails and needs to be restarted, the system
     tries to schedule it at the highest priority. If there are no idle Compute
     Nodes available, the system may terminate one of the running Tasks in the Pool
     and return it to the queue in order to make room for the Job Manager Task to
     restart. Note that a Job Manager Task in one Job does not have priority over
     Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For
     example, if a Job Manager in a priority 0 Job needs to be restarted, it will
     not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery
     operation is triggered on a Node. Examples of recovery operations include (but
     are not limited to) when an unhealthy Node is rebooted or a Compute Node
     disappeared due to host failure. Retries due to recovery operations are
     independent of and are not counted against the maxTaskRetryCount. Even if the
     maxTaskRetryCount is 0, an internal retry due to a recovery operation may
     occur. Because of this, all Tasks should be idempotent. This means Tasks need
     to tolerate being interrupted and restarted without causing any corruption or
     duplicate data. The best practice for long running Tasks is to use some form of
     checkpointing.
    :vartype job_manager_task: str
    :ivar job_preparation_task: The Job Preparation Task is a special Task run on each Compute Node
     before any
     other Task of the Job.
    :vartype job_preparation_task: str
    :ivar job_release_task: The Job Release Task is a special Task run at the end of the Job on
     each
     Compute Node that has run any other Task of the Job.
    :vartype job_release_task: str
    :ivar common_environment_settings: Individual Tasks can override an environment setting
     specified here by
     specifying the same setting name with a different value.
    :vartype common_environment_settings: list[str]
    :ivar pool_info: Specifies how a Job should be assigned to a Pool.
    :vartype pool_info: str
    :ivar on_all_tasks_complete: The default is noaction.
    :vartype on_all_tasks_complete: str
    :ivar on_task_failure: A Task is considered to have failed if has a failureInfo. A failureInfo
     is set
     if the Task completes with a non-zero exit code after exhausting its retry
     count, or if there was an error starting the Task, for example due to a
     resource file download error. The default is noaction.
    :vartype on_task_failure: str
    :ivar network_configuration: The network configuration for the Job.
    :vartype network_configuration: str
    :ivar metadata: The Batch service does not assign any meaning to metadata; it is solely for the
     use of user code.
    :vartype metadata: list[str]
    :ivar execution_info: Contains information about the execution of a Job in the Azure Batch
     service.
    :vartype execution_info: str
    :ivar stats: This property is populated only if the CloudJob was retrieved with an expand
     clause including the 'stats' attribute; otherwise it is null. The statistics
     may not be immediately available. The Batch service performs periodic roll-up
     of statistics. The typical delay is about 30 minutes.
    :vartype stats: str
    """

    id: str = rest_field()
    """The ID is case-preserving and case-insensitive (that is, you may not have two
     IDs within an Account that differ only by case). Required."""
    display_name: Optional[str] = rest_field(name="displayName")
    """The display name for the Job."""
    uses_task_dependencies: Optional[bool] = rest_field(name="usesTaskDependencies")
    """Whether Tasks in the Job can define dependencies on each other. The default is
     false."""
    url: Optional[str] = rest_field(readonly=True)
    """The URL of the Job."""
    e_tag: Optional[str] = rest_field(name="eTag", readonly=True)
    """This is an opaque string. You can use it to detect whether the Job has changed
     between requests. In particular, you can be pass the ETag when updating a Job
     to specify that your changes should take effect only if nobody else has
     modified the Job in the meantime."""
    last_modified: Optional[datetime.datetime] = rest_field(name="lastModified", readonly=True)
    """This is the last time at which the Job level data, such as the Job state or
     priority, changed. It does not factor in task-level changes such as adding new
     Tasks or Tasks changing state."""
    creation_time: Optional[datetime.datetime] = rest_field(name="creationTime", readonly=True)
    """The creation time of the Job."""
    state: Optional[str] = rest_field(readonly=True)
    """The state of the Job."""
    state_transition_time: Optional[datetime.datetime] = rest_field(name="stateTransitionTime", readonly=True)
    """The time at which the Job entered its current state."""
    previous_state: Optional[str] = rest_field(name="previousState", readonly=True)
    """This property is not set if the Job is in its initial Active state."""
    previous_state_transition_time: Optional[datetime.datetime] = rest_field(
        name="previousStateTransitionTime", readonly=True
    )
    """This property is not set if the Job is in its initial Active state."""
    priority: Optional[int] = rest_field()
    """Priority values can range from -1000 to 1000, with -1000 being the lowest
     priority and 1000 being the highest priority. The default value is 0."""
    allow_task_preemption: Optional[bool] = rest_field(name="allowTaskPreemption")
    """If the value is set to True, other high priority jobs submitted to the system
     will take precedence and will be able requeue tasks from this job. You can
     update a job's allowTaskPreemption after it has been created using the update
     job API."""
    max_parallel_tasks: Optional[int] = rest_field(name="maxParallelTasks")
    """The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
     specified, the default value is -1, which means there's no limit to the number
     of tasks that can be run at once. You can update a job's maxParallelTasks after
     it has been created using the update job API."""
    constraints: Optional[str] = rest_field()
    """The execution constraints for a Job."""
    job_manager_task: Optional[str] = rest_field(name="jobManagerTask")
    """The Job Manager Task is automatically started when the Job is created. The
     Batch service tries to schedule the Job Manager Task before any other Tasks in
     the Job. When shrinking a Pool, the Batch service tries to preserve Nodes where
     Job Manager Tasks are running for as long as possible (that is, Compute Nodes
     running 'normal' Tasks are removed before Compute Nodes running Job Manager
     Tasks). When a Job Manager Task fails and needs to be restarted, the system
     tries to schedule it at the highest priority. If there are no idle Compute
     Nodes available, the system may terminate one of the running Tasks in the Pool
     and return it to the queue in order to make room for the Job Manager Task to
     restart. Note that a Job Manager Task in one Job does not have priority over
     Tasks in other Jobs. Across Jobs, only Job level priorities are observed. For
     example, if a Job Manager in a priority 0 Job needs to be restarted, it will
     not displace Tasks of a priority 1 Job. Batch will retry Tasks when a recovery
     operation is triggered on a Node. Examples of recovery operations include (but
     are not limited to) when an unhealthy Node is rebooted or a Compute Node
     disappeared due to host failure. Retries due to recovery operations are
     independent of and are not counted against the maxTaskRetryCount. Even if the
     maxTaskRetryCount is 0, an internal retry due to a recovery operation may
     occur. Because of this, all Tasks should be idempotent. This means Tasks need
     to tolerate being interrupted and restarted without causing any corruption or
     duplicate data. The best practice for long running Tasks is to use some form of
     checkpointing."""
    job_preparation_task: Optional[str] = rest_field(name="jobPreparationTask")
    """The Job Preparation Task is a special Task run on each Compute Node before any
     other Task of the Job."""
    job_release_task: Optional[str] = rest_field(name="jobReleaseTask")
    """The Job Release Task is a special Task run at the end of the Job on each
     Compute Node that has run any other Task of the Job."""
    common_environment_settings: Optional[List[str]] = rest_field(name="commonEnvironmentSettings")
    """Individual Tasks can override an environment setting specified here by
     specifying the same setting name with a different value."""
    pool_info: Optional[str] = rest_field(name="poolInfo")
    """Specifies how a Job should be assigned to a Pool."""
    on_all_tasks_complete: Optional[str] = rest_field(name="onAllTasksComplete")
    """The default is noaction."""
    on_task_failure: Optional[str] = rest_field(name="onTaskFailure")
    """A Task is considered to have failed if has a failureInfo. A failureInfo is set
     if the Task completes with a non-zero exit code after exhausting its retry
     count, or if there was an error starting the Task, for example due to a
     resource file download error. The default is noaction."""
    network_configuration: Optional[str] = rest_field(name="networkConfiguration")
    """The network configuration for the Job."""
    metadata: Optional[List[str]] = rest_field()
    """The Batch service does not assign any meaning to metadata; it is solely for the
     use of user code."""
    execution_info: Optional[str] = rest_field(name="executionInfo", readonly=True)
    """Contains information about the execution of a Job in the Azure Batch service."""
    stats: Optional[str] = rest_field(readonly=True)
    """This property is populated only if the CloudJob was retrieved with an expand
     clause including the 'stats' attribute; otherwise it is null. The statistics
     may not be immediately available. The Batch service performs periodic roll-up
     of statistics. The typical delay is about 30 minutes."""

    @overload
    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        display_name: Optional[str] = None,
        uses_task_dependencies: Optional[bool] = None,
        priority: Optional[int] = None,
        allow_task_preemption: Optional[bool] = None,
        max_parallel_tasks: Optional[int] = None,
        constraints: Optional[str] = None,
        job_manager_task: Optional[str] = None,
        job_preparation_task: Optional[str] = None,
        job_release_task: Optional[str] = None,
        common_environment_settings: Optional[List[str]] = None,
        pool_info: Optional[str] = None,
        on_all_tasks_complete: Optional[str] = None,
        on_task_failure: Optional[str] = None,
        network_configuration: Optional[str] = None,
        metadata: Optional[List[str]] = None,
    ):
        ...

    @overload
    def __init__(self, mapping: Mapping[str, Any]):
        """
        :param mapping: raw JSON to initialize the model.
        :type mapping: Mapping[str, Any]
        """

    def __init__(self, *args: Any, **kwargs: Any) -> None:  # pylint: disable=useless-super-delegation
        super().__init__(*args, **kwargs)
